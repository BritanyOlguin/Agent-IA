import os
import pandas as pd
import tqdm
import torch
import time
import gc
import pyodbc
from llama_index.core import VectorStoreIndex
from llama_index.core.schema import Document
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.settings import Settings
import unicodedata
import re
import sys
# Importar la funci√≥n normalizar_texto desde normalizar_texto.py
sys.path.append(r"C:\Users\Sistemas\Documents\OKIP\src")
from normalizar_texto import normalizar_texto

def crear_nombre_completo(row):
    """
    Crea un nombre completo a partir de columnas si existen nombre, apellido paterno y materno.
    """
    columnas_nombre = ['nombre', 'NOMBRE']
    columnas_apellido_paterno = ['apellido_paterno', 'apellido_p', 'a_paterno', 'PATERNO']
    columnas_apellido_materno = ['apellido_materno', 'apellido_m', 'a_materno', 'MATERNO']

    columnas_df = [col.lower() for col in row.index]
    col_nombre = col_apellido_paterno = col_apellido_materno = None

    for nombre_pos in columnas_nombre:
        for col in columnas_df:
            if nombre_pos.lower() in col:
                col_nombre = row.index[columnas_df.index(col)]
                break
        if col_nombre:
            break

    for apellido_pos in columnas_apellido_paterno:
        for col in columnas_df:
            if apellido_pos.lower() in col:
                col_apellido_paterno = row.index[columnas_df.index(col)]
                break
        if col_apellido_paterno:
            break

    for apellido_pos in columnas_apellido_materno:
        for col in columnas_df:
            if apellido_pos.lower() in col:
                col_apellido_materno = row.index[columnas_df.index(col)]
                break
        if col_apellido_materno:
            break

    if col_nombre and col_apellido_paterno and col_apellido_materno:
        partes = [row[col_nombre], row[col_apellido_paterno], row[col_apellido_materno]]
        if all(str(p).strip() for p in partes):
            return " ".join(str(p).strip() for p in partes)
    
    return None

# --- CONFIGURACI√ìN GLOBAL ---
ruta_modelo_embeddings = r"C:\Users\Sistemas\Documents\OKIP\models\models--intfloat--e5-large-v2"
carpeta_bd = r"C:\Users\Sistemas\Documents\OKIP\archivos"
carpeta_indices = r"C:\Users\Sistemas\Documents\OKIP\llama_index_indices"

os.makedirs(carpeta_indices, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üíª Dispositivo de embeddings: {device}")

Settings.embed_model = HuggingFaceEmbedding(
    model_name=ruta_modelo_embeddings,
    device=device,
    normalize=True
)

def cargar_dataframe(archivo):
    ext = os.path.splitext(archivo)[1].lower()
    print(f"üìÅ Detectado archivo: {archivo} (tipo: {ext})")
    try:
        if ext in ['.xlsx', '.xls']:
            return pd.read_excel(archivo, dtype=str, engine="openpyxl").fillna("")
        elif ext in ['.accdb', '.mdb']:
            conn_str = (
                r"DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};"
                f"DBQ={archivo};"
            )
            conn = pyodbc.connect(conn_str)
            cursor = conn.cursor()
            tablas = [row.table_name for row in cursor.tables(tableType='TABLE')]
            if not tablas:
                raise ValueError("No se encontraron tablas en la base de datos Access.")
            print(f"üìå Usando tabla: {tablas[0]}")
            df = pd.read_sql(f"SELECT * FROM {tablas[0]}", conn).fillna("")
            conn.close()
            return df
        elif ext == '.dbf':
            from dbfread import DBF
            table = DBF(archivo, load=True)
            df = pd.DataFrame(iter(table)).fillna("")
            return df
        elif ext == '.csv':
            # Intentar con diferentes codificaciones y delimitadores comunes
            try:
                # Primero intentar con UTF-8 y delimitador ',' (est√°ndar)
                return pd.read_csv(archivo, dtype=str, encoding='utf-8', delimiter=',').fillna("")
            except UnicodeDecodeError:
                # Si falla, intentar con latin-1 (com√∫n para datos en espa√±ol)
                return pd.read_csv(archivo, dtype=str, encoding='latin-1', delimiter=',').fillna("")
            except:
                # Si a√∫n falla, intentar con otros delimitadores (punto y coma es com√∫n en pa√≠ses europeos)
                try:
                    return pd.read_csv(archivo, dtype=str, encoding='utf-8', delimiter=';').fillna("")
                except:
                    return pd.read_csv(archivo, dtype=str, encoding='latin-1', delimiter=';').fillna("")
        # A√±adir soporte para TXT
        elif ext == '.txt':
            print("üìÑ Procesando archivo TXT...")
            
            # Intentar detectar la estructura del archivo
            try:
                # Primero intentar leer como CSV por si es un archivo delimitado
                for encoding in ['utf-8', 'latin-1']:
                    for delimiter in [',', ';', '\t', '|']:
                        try:
                            df = pd.read_csv(archivo, dtype=str, encoding=encoding, delimiter=delimiter)
                            if len(df.columns) > 1:  # Si tiene m√°s de una columna, probablemente es un CSV
                                print(f"‚úÖ Archivo TXT le√≠do como CSV con delimitador '{delimiter}'")
                                return df.fillna("")
                        except:
                            continue
                
                # Si no se pudo leer como CSV, intentar detectar formato de l√≠neas clave-valor
                with open(archivo, 'r', encoding='utf-8') as f:
                    contenido = f.readlines()
                
                # Ver si las l√≠neas tienen formato "clave: valor" o "clave=valor"
                pares_clave_valor = []
                for linea in contenido:
                    linea = linea.strip()
                    if not linea:
                        continue
                    
                    # Intentar varios separadores comunes
                    for sep in [':', '=', '\t']:
                        if sep in linea:
                            partes = linea.split(sep, 1)
                            if len(partes) == 2:
                                clave = partes[0].strip()
                                valor = partes[1].strip()
                                pares_clave_valor.append((clave, valor))
                                break
                
                if pares_clave_valor:
                    # Convertir a DataFrame
                    print("‚úÖ Archivo TXT interpretado como pares clave-valor")
                    df_dict = {}
                    for clave, valor in pares_clave_valor:
                        df_dict[clave] = [valor]
                    return pd.DataFrame(df_dict).fillna("")
                
                # Si todo lo anterior falla, leer como texto plano y crear un DataFrame simple
                print("‚ÑπÔ∏è No se detect√≥ estructura. Procesando como texto plano.")
                with open(archivo, 'r', encoding='utf-8') as f:
                    texto = f.read()
                
                # Crear un DataFrame con una sola fila y una columna "texto"
                return pd.DataFrame({"texto": [texto]}).fillna("")
                
            except UnicodeDecodeError:
                # Si falla con UTF-8, intentar con latin-1
                try:
                    with open(archivo, 'r', encoding='latin-1') as f:
                        texto = f.read()
                    return pd.DataFrame({"texto": [texto]}).fillna("")
                except Exception as e:
                    print(f"‚ùå Error al procesar archivo TXT: {e}")
                    return None
            except Exception as e:
                print(f"‚ùå Error al procesar archivo TXT: {e}")
                return None
        else:
            print(f"‚ö†Ô∏è Formato no soportado: {ext}")
            return None
    except Exception as e:
        print(f"‚ùå Error al cargar archivo {archivo}: {e}")
        return None

# --- RECORRER CARPETA Y PROCESAR ARCHIVOS ---
for archivo_nombre in os.listdir(carpeta_bd):
    ruta_archivo = os.path.join(carpeta_bd, archivo_nombre)
    if not os.path.isfile(ruta_archivo):
        continue

    nombre_archivo = os.path.basename(ruta_archivo)
    nombre_fuente = os.path.splitext(nombre_archivo)[0]
    ruta_llama_index = os.path.join(carpeta_indices, f"index_{nombre_fuente}")
    os.makedirs(ruta_llama_index, exist_ok=True)

    print(f"\n============================")
    print(f"üì¶ Procesando: {nombre_archivo}")
    print(f"üìå Fuente: {nombre_fuente}")
    print(f"üìÇ √çndice: {ruta_llama_index}")

    df = cargar_dataframe(ruta_archivo)
    if df is None or df.empty:
        print("‚ö†Ô∏è Archivo vac√≠o o no se pudo procesar. Saltando...")
        continue

    total_filas = len(df)
    batch_size = 100
    print(f"üìÑ Filas a procesar: {total_filas}")

    documentos_totales = []
    with tqdm.tqdm(total=total_filas, desc="üìÑ Preparando documentos", unit=" filas") as pbar:
        # Modificar la preparaci√≥n de documentos
        for i, row in df.iterrows():
            # 1. Generar nombre completo si es posible
            nombre_completo_raw = crear_nombre_completo(row)
            nombre_completo = normalizar_texto(nombre_completo_raw) if nombre_completo_raw else None

            # 2. Convertir todo a texto limpio y normalizado
            datos_fila = {col: str(row[col]).strip() for col in df.columns}
            datos_fila_limpios = {
                normalizar_texto(col): normalizar_texto(v) 
                for col, v in datos_fila.items() 
                if not pd.isna(v) and str(v).lower() not in ["nan", "3586127"]
            }

            # 3. Si se gener√≥ nombre completo, eliminar nombre, paterno, materno
            if nombre_completo:
                for key in list(datos_fila_limpios.keys()):
                    if any(x in key for x in ["nombre", "apellido_p", "apellido_m", "a_paterno", "a_materno", "paterno", "materno"]):
                        del datos_fila_limpios[key]

            # 4. Generar texto e incluir nombre_completo si aplica
            texto_base = "\n".join([f"{col}: {v}" for col, v in datos_fila_limpios.items() if v])
            texto_a_incrustar = texto_base
            if nombre_completo:
                texto_a_incrustar += f"\nnombre_completo: {nombre_completo}"

            # Generar campo direccion_completa
            componentes_direccion = []
            componentes_encontrados = {}
            campos_direccion = ['calle', 'domicilio', 'numero', 'campo 14', 'colonia', 'cp', 'codigo postal', 'municipio', 'ciudad', 'sector', 'estado', 'edo de origen', 'entidad']

            for campo in campos_direccion:
                for col in datos_fila_limpios.keys():
                    if campo in col:
                        valor = datos_fila_limpios[col]
                        if valor and valor.lower() != "nan":
                            componentes_encontrados[campo] = valor

            direccion_partes = []

            if 'domicilio' in componentes_encontrados:
                domicilio = componentes_encontrados['domicilio']
                # Si hay un n√∫mero, a√±adirlo con espacio
                if 'numero' in componentes_encontrados:
                    domicilio += " " + componentes_encontrados['numero']
                    componentes_encontrados.pop('numero')  # Eliminar n√∫mero para no agregarlo dos veces
                direccion_partes.append(domicilio)
            elif 'calle' in componentes_encontrados:
                calle = componentes_encontrados['calle']
                # Si hay un n√∫mero, a√±adirlo con espacio
                if 'numero' in componentes_encontrados:
                    calle += " " + componentes_encontrados['numero']
                    componentes_encontrados.pop('numero')  # Eliminar n√∫mero para no agregarlo dos veces
                direccion_partes.append(calle)

            for campo in campos_direccion:
                if campo not in ['domicilio', 'calle', 'numero'] and campo in componentes_encontrados:
                    direccion_partes.append(componentes_encontrados[campo])

            # Unir todos los componentes con comas
            direccion_completa = ", ".join(direccion_partes) if direccion_partes else None

            if direccion_completa:
                direccion_completa = normalizar_texto(direccion_completa)
                datos_fila_limpios["direccion"] = direccion_completa
                texto_a_incrustar += f"\ndireccion: {direccion_completa}"


            # 5. Generar metadata, tambi√©n condicional
            metadata = {
                "fuente": nombre_fuente,
                "archivo": nombre_archivo,
                "fila_origen": i,
                **datos_fila_limpios
            }
            if nombre_completo:
                metadata["nombre_completo"] = nombre_completo

            # 6. Guardar el documento
            documentos_totales.append(Document(text=texto_a_incrustar, metadata=metadata))
            pbar.update(1)

    if documentos_totales:
        print(f"‚öôÔ∏è Indexando {len(documentos_totales)} documentos...")
        start_time_indexing = time.time()
        index = VectorStoreIndex(
            documentos_totales,
            embed_model=Settings.embed_model,
            embed_batch_size=32,
            show_progress=True
        )
        index.storage_context.persist(persist_dir=ruta_llama_index)
        print(f"‚úÖ Indexaci√≥n completada en {time.time() - start_time_indexing:.2f} segundos.")
    else:
        print("‚ö†Ô∏è No se generaron documentos.")

    del df, documentos_totales, index
    if device == "cuda":
        torch.cuda.empty_cache()
    gc.collect()
